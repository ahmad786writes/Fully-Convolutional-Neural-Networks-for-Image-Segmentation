{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500aec7d-95ff-4cf8-8f1d-0a77aa7e727e",
   "metadata": {},
   "source": [
    "## Fully Convolutional Neural Networks for Image Segmentation\n",
    "\n",
    "we will train the model on a [custom dataset](https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view?usp=sharing) prepared by [divamgupta](https://github.com/divamgupta/image-segmentation-keras). This contains video frames from a moving vehicle and is a subsample of the [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/) dataset.\n",
    "\n",
    "we will be using a pretrained VGG-16 network for the feature extraction path, then followed by an FCN-8 network for upsampling and generating the predictions. The output will be a label map (i.e. segmentation mask) with predictions for 12 classes. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb298d-10c6-45d0-928a-bb544eb70ab9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f652895-6997-417f-9c6c-d56cadcb6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Tensorflow Version\" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf955cb2-f923-4b99-93c2-f1550cd87b5e",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30793a2-584b-4b00-9a0d-451c3a96cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset (zipped file)\n",
    "# !wget https://storage.googleapis.com/learning-datasets/fcnn-dataset.zip  -O fcnn-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a1e1e-c8c6-4125-8112-e1b40a6ef64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the downloaded dataset to a local directory\n",
    "local_zip = 'fcnn-dataset.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('fcnn')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872855ad-52de-442e-aa21-a529e4c8539f",
   "metadata": {},
   "source": [
    "The dataset you just downloaded contains folders for images and annotations. The *images* contain the video frames while the *annotations* contain the pixel-wise label maps. Each label map has the shape `(height, width , 1)` with each point in this space denoting the corresponding pixel's class. Classes are in the range `[0, 11]` (i.e. 12 classes) and the pixel labels correspond to these classes:\n",
    "\n",
    "| Value  | Class Name    |\n",
    "| -------| -------------|\n",
    "| 0      | sky |\n",
    "| 1      | building      |\n",
    "| 2      | column/pole      |\n",
    "| 3      | road |\n",
    "| 4      | side walk     |\n",
    "| 5      | vegetation      |\n",
    "| 6      | traffic light |\n",
    "| 7      | fence      |\n",
    "| 8      | vehicle     |\n",
    "| 9      | pedestrian |\n",
    "| 10      | byciclist      |\n",
    "| 11      | void      |\n",
    "\n",
    "For example, if a pixel is part of a road, then that point will be labeled `3` in the label map. Run the cell below to create a list containing the class names:\n",
    "- Note: bicyclist is mispelled as 'byciclist' in the dataset.  We won't handle data cleaning in this example, but you can inspect and clean the data if you want to use this as a starting point for a personal project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc9184-da0c-49a0-b881-4916e8169968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel labels in the video frames\n",
    "class_names = ['sky', 'building','column/pole', 'road', 'side walk', 'vegetation', 'traffic light', 'fence', 'vehicle', 'pedestrian', 'byciclist', 'void']\n",
    "len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf71c78-34b3-462b-9b71-9d1402b9d426",
   "metadata": {},
   "source": [
    "## Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461742f-b28b-4681-bc0b-4397ac93bca5",
   "metadata": {},
   "source": [
    "\n",
    "Next, you will load and prepare the train and validation sets for training. There are some preprocessing steps needed before the data is fed to the model. These include:\n",
    "\n",
    "* resizing the height and width of the input images and label maps (224 x 224px by default)\n",
    "* normalizing the input images' pixel values to fall in the range `[-1, 1]`\n",
    "* reshaping the label maps from `(height, width, 1)` to `(height, width, 12)` with each slice along the third axis having `1` if it belongs to the class corresponding to that slice's index else `0`. For example, if a pixel is part of a road, then using the table above, that point at slice #3 will be labeled `1` and it will be `0` in all other slices. To illustrate using simple arrays:\n",
    "```\n",
    "# if we have a label map with 3 classes...\n",
    "n_classes = 3\n",
    "# and this is the original annotation...\n",
    "orig_anno = [0 1 2]\n",
    "# then the reshaped annotation will have 3 slices and its contents will look like this:\n",
    "reshaped_anno = [1 0 0][0 1 0][0 0 1]\n",
    "```\n",
    "\n",
    "The following function will do the preprocessing steps mentioned above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994c875-1dfb-4aeb-b7e3-b6defdc0654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
    "\n",
    "    # Convert image and mask files to tensors\n",
    "    img_raw = tf.io.read_file(t_filename)\n",
    "    anno_raw = tf.io.read_file(a_filename)\n",
    "    image = tf.image.decode_jpeg(img_raw)\n",
    "    annotation = tf.image.decode_jpeg(anno_raw)\n",
    "\n",
    "    # Resize image and segmentation mask\n",
    "    image = tf.image.resize(image, (height, width))\n",
    "    annotation = tf.image.resize(annotation, (height, width))\n",
    "    image = tf.reshape(image, (height, width, 3,))\n",
    "    annotation = tf.cast(annotation, dtype=tf.int32)\n",
    "    annotation = tf.reshape(annotation, (height, width, 1,))\n",
    "    stack_list = []\n",
    "\n",
    "    # Reshape segmentation masks\n",
    "    for i in range(len(class_names)):\n",
    "        mask = tf.equal(annotation[:,:,0], tf.constant(i))\n",
    "        stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
    "        \n",
    "    annotation = tf.stack(stack_list, axis=2)\n",
    "\n",
    "    # Normalize pixels in the input image\n",
    "    image = image / 127.5\n",
    "    image -= 1\n",
    "    \n",
    "    return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced5ea2-86d0-4263-a67e-2b6a11d02a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('fcnn/dataset1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32e747-e935-4772-8cb1-4a46eb45c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "def get_dataset_slice_paths(image_dir, label_dir):\n",
    "\n",
    "    image_file_list = os.listdir(image_dir)\n",
    "    label_map_dir = os.listdir(label_dir)\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "    labels_map_paths = [os.path.join(label_dir, fname) for fname in label_map_dir]\n",
    "\n",
    "    return image_paths, labels_map_paths\n",
    "\n",
    "def get_training_dataset(image_paths, label_map_paths):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "    dataset = dataset.map(map_filename_to_image_and_mask)\n",
    "    dataset = dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(-1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(image_paths, labels_map_paths):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_map_paths))\n",
    "    dataset = dataset.map(map_filename_to_image_and_mask)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81c616-9a28-48eb-abd8-9f6cb119e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths, training_label_paths = get_dataset_slice_paths('fcnn/dataset1/images_prepped_train', 'fcnn/dataset1/annotations_prepped_train')\n",
    "validation_image_paths, validation_label_paths = get_dataset_slice_paths('fcnn/dataset1/images_prepped_test', 'fcnn/dataset1/annotations_prepped_test')\n",
    "training_dataset = get_training_dataset(training_image_paths, training_label_paths)\n",
    "validation_dataset = get_validation_dataset(validation_image_paths, validation_label_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767244e8-6808-41fd-aa6e-b4b5cf6a9ef9",
   "metadata": {},
   "source": [
    "## Let's Take a Look at the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accbc29-bfac-400b-a58f-dca3568ba402",
   "metadata": {},
   "source": [
    "You will also need utilities to help visualize the dataset and the model predictions later. First, you need to assign a color mapping to the classes in the label maps. Since our dataset has 12 classes, you need to have a list of 12 colors. We can use the color_palette() from Seaborn to generate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e561dd-30d3-499d-9172-ba15781a3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list that contains one color for each class\n",
    "colors = sns.color_palette(None, len(class_names))\n",
    "\n",
    "for class_names, color in zip(class_names, colors):\n",
    "    print(f'{class_names}--{color}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cda49-1ba5-4a78-a995-f575baf1a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Utilities\n",
    "\n",
    "def fuse_with_pil(images):\n",
    "    '''\n",
    "    Creates a blank image and pastes input images\n",
    "    \n",
    "    Args:\n",
    "    images (list of numpy arrays) - numpy array representations of the images to paste\n",
    "    \n",
    "    Returns:\n",
    "    PIL Image object containing the images\n",
    "    '''\n",
    "    \n",
    "    widths = (image.shape[1] for image in images)\n",
    "    heights = (image.shape[0] for image in images)\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "    \n",
    "    new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
    "    \n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        pil_image = PIL.Image.fromarray(np.uint8(im))\n",
    "        new_im.paste(pil_image, (x_offset,0))\n",
    "        x_offset += im.shape[1]\n",
    "    \n",
    "    return new_im\n",
    "\n",
    "\n",
    "def give_color_to_annotation(annotation):\n",
    "    '''\n",
    "    Converts a 2-D annotation to a numpy array with shape (height, width, 3) where\n",
    "    the third axis represents the color channel. The label values are multiplied by\n",
    "    255 and placed in this axis to give color to the annotation\n",
    "    \n",
    "    Args:\n",
    "    annotation (numpy array) - label map array\n",
    "    \n",
    "    Returns:\n",
    "    the annotation array with an additional color channel/axis\n",
    "    '''\n",
    "    seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
    "    \n",
    "    for c in range(12):\n",
    "        segc = (annotation == c)\n",
    "        seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
    "        seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
    "        seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "\n",
    "def show_predictions(image, labelmaps, titles, iou_list, dice_score_list):\n",
    "    '''\n",
    "    Displays the images with the ground truth and predicted label maps\n",
    "    \n",
    "    Args:\n",
    "    image (numpy array) -- the input image\n",
    "    labelmaps (list of arrays) -- contains the predicted and ground truth label maps\n",
    "    titles (list of strings) -- display headings for the images to be displayed\n",
    "    iou_list (list of floats) -- the IOU values for each class\n",
    "    dice_score_list (list of floats) -- the Dice Score for each vlass\n",
    "    '''\n",
    "    \n",
    "    true_img = give_color_to_annotation(labelmaps[1])\n",
    "    pred_img = give_color_to_annotation(labelmaps[0])\n",
    "    \n",
    "    image = image + 1\n",
    "    image = image * 127.5\n",
    "    images = np.uint8([image, pred_img, true_img])\n",
    "    \n",
    "    metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n",
    "    metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
    "    print(metrics_by_id)\n",
    "    \n",
    "    display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
    "    display_string = \"\\n\\n\".join(display_string_list)\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    for idx, im in enumerate(images):\n",
    "        plt.subplot(1, 3, idx+1)\n",
    "        if idx == 1:\n",
    "          plt.xlabel(display_string)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(titles[idx], fontsize=12)\n",
    "        plt.imshow(im)\n",
    "    \n",
    "\n",
    "def show_annotation_and_image(image, annotation):\n",
    "    '''\n",
    "    Displays the image and its annotation side by side\n",
    "    \n",
    "    Args:\n",
    "    image (numpy array) -- the input image\n",
    "    annotation (numpy array) -- the label map\n",
    "    '''\n",
    "    new_ann = np.argmax(annotation, axis=2)\n",
    "    seg_img = give_color_to_annotation(new_ann)\n",
    "    \n",
    "    image = image + 1\n",
    "    image = image * 127.5\n",
    "    image = np.uint8(image)\n",
    "    images = [image, seg_img]\n",
    "    \n",
    "    images = [image, seg_img]\n",
    "    fused_img = fuse_with_pil(images)\n",
    "    plt.imshow(fused_img)\n",
    "\n",
    "\n",
    "def list_show_annotation(dataset):\n",
    "    '''\n",
    "    Displays images and its annotations side by side\n",
    "    \n",
    "    Args:\n",
    "    dataset (tf Dataset) - batch of images and annotations\n",
    "    '''\n",
    "    \n",
    "    ds = dataset.unbatch()\n",
    "    ds = ds.shuffle(buffer_size=100)\n",
    "    \n",
    "    plt.figure(figsize=(25, 15))\n",
    "    plt.title(\"Images And Annotations\")\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
    "    \n",
    "    # we set the number of image-annotation pairs to 9\n",
    "    # feel free to make this a function parameter if you want\n",
    "    for idx, (image, annotation) in enumerate(ds.take(9)):\n",
    "        plt.subplot(3, 3, idx + 1)\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        show_annotation_and_image(image.numpy(), annotation.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a93dc8-33e9-4610-8d38-bf5d43ad6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_show_annotation(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee0fdf-3fbb-46b9-8607-bae9156d8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_show_annotation(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a446f3b-7438-4019-97e4-42179b70c397",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "## Define the Model\n",
    "\n",
    "You will now build the model and prepare it for training. AS mentioned earlier, this will use a VGG-16 network for the encoder and FCN-8 for the decoder. This is the diagram as shown in class:\n",
    "\n",
    "<img src='fcn-8 architecture.png' alt='fcn-8'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ad4f6-fe52-470f-a6c8-0663080a19a4",
   "metadata": {},
   "source": [
    "## Define Pooling Block of VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9a48e-8a04-4ca7-ab04-a0d5947c32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(tf.keras.models.Model):\n",
    "    def __init__(self, n_convs, filters, kernal_size, activation, pool_size, pool_stride, block_name):\n",
    "        super(block, self).__init__()\n",
    "        self.n_convs = n_convs\n",
    "        self.conv2d_module = []\n",
    "        for i in range(n_convs):\n",
    "            self.conv2d_module.append(tf.keras.layers.Conv2D(filters=filters,\n",
    "                                                             kernel_size=kernal_size,\n",
    "                                                             activation=activation,\n",
    "                                                             padding='same',\n",
    "                                                             name='{}_conv{}'.format(block_name, i+1)))\n",
    "        self.pooling_layer = tf.keras.layers.MaxPooling2D(pool_size=pool_size,\n",
    "                                                          strides=pool_stride,\n",
    "                                                          name='{}_pool{}'.format(block_name, 1))\n",
    "\n",
    "    def call(self, x):\n",
    "        for i in range(self.n_convs):\n",
    "            x = self.conv2d_module[i](x)\n",
    "        x = self.pooling_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e003d-b7af-4881-adf1-8b838fbbe812",
   "metadata": {},
   "source": [
    "## Define VGG-16\n",
    "\n",
    "You can build the encoder as shown below.\n",
    "\n",
    "* You will create 5 blocks with increasing number of filters at each stage.\n",
    "* The number of convolutions, filters, kernel size, activation, pool size and pool stride will remain constant.\n",
    "* You will load the pretrained weights after creating the VGG 16 network.\n",
    "* Additional convolution layers will be appended to extract more features.\n",
    "* The output will contain the output of the last layer and the previous four convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b0ec2-2bd4-4251-a57c-f6283b511395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_16(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG_16, self).__init__()\n",
    "    \n",
    "        self.b1 = block(n_convs=2,\n",
    "                    filters=64,\n",
    "                    kernal_size=(3,3),\n",
    "                    activation='relu',\n",
    "                    pool_size=(2,2),\n",
    "                    pool_stride=(2,2),\n",
    "                    block_name='block1')\n",
    "        self.b2 = block(n_convs=2,\n",
    "                    filters=128,\n",
    "                    kernal_size=(3,3),\n",
    "                    activation='relu',\n",
    "                    pool_size=(2,2),\n",
    "                    pool_stride=(2,2),\n",
    "                    block_name='block1')\n",
    "        self.b3 = block(n_convs=3,\n",
    "                    filters=256,\n",
    "                    kernal_size=(3,3),\n",
    "                    activation='relu',\n",
    "                    pool_size=(2,2),\n",
    "                    pool_stride=(2,2),\n",
    "                    block_name='block1')\n",
    "        self.b4 = block(n_convs=3,\n",
    "                    filters=512,\n",
    "                    kernal_size=(3,3),\n",
    "                    activation='relu',\n",
    "                    pool_size=(2,2),\n",
    "                    pool_stride=(2,2),\n",
    "                    block_name='block1')\n",
    "        self.b5 = block(n_convs=3,\n",
    "                    filters=512,\n",
    "                    kernal_size=(3,3),\n",
    "                    activation='relu',\n",
    "                    pool_size=(2,2),\n",
    "                    pool_stride=(2,2),\n",
    "                    block_name='block1')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        p1 = self.b1(inputs)\n",
    "        x = p1\n",
    "        p2 = self.b2(x)\n",
    "        x = p2\n",
    "        p3 = self.b3(x)\n",
    "        x = p3\n",
    "        p4 = self.b4(x)\n",
    "        x = p4\n",
    "        p5 = self.b5(x)\n",
    "        return (p1, p2, p3, p4, p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2146a-391b-45db-a0a5-f42adb3cfb4e",
   "metadata": {},
   "source": [
    "## Download VGG weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f61393-5e87-455f-87b2-d6338a90697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the weights\n",
    "# !wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adceef6-3e44-4fe6-a03c-8a86c2a0e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_vgg_16_encoder(image_input):\n",
    "#     weights_path = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "#     # create the vgg model\n",
    "#     vgg16_model = VGG_16()\n",
    "#     # load the pretrained weights you downloaded earlier\n",
    "#     vgg16_model.load_weights(weights_path, by_name=True)\n",
    "#     (p1,p2,p3,p4,p5) = vgg16_model(image_input)\n",
    "    \n",
    "#     # number of filters for the output convolutional layers\n",
    "#     n = 4096\n",
    "#     # our input images are 224x224 pixels so they will be downsampled to 7x7 after the pooling layers above.\n",
    "#     # we can extract more features by chaining two more convolution layers.\n",
    "#     c6 = tf.keras.layers.Conv2D(n, kernel_size=(7,7), padding='same', activation='relu', name='conv6')(p5)\n",
    "#     c7 = tf.keras.layers.Conv2D(n, kernel_size=(1,1), padding='same', activation='relu', name='conv7')(c6)\n",
    "#     return (p1,p2,p3,p4,c7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6948d-d117-4571-a8d9-bd3fa3bfe883",
   "metadata": {},
   "source": [
    "## Define FCN 8 Decoder\n",
    "\n",
    "Next, you will build the decoder using deconvolution layers. Please refer to the diagram for FCN-8 at the start of this section to visualize what the code below is doing. It will involve two summations before upsampling to the original image size and generating the predicted mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e01c0-0dbf-42c9-9a80-173130e4a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_8(tf.keras.models.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN_8, self).__init__()\n",
    "        self.x2_upsample_1 = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4,4), strides=(2,2), use_bias=False)\n",
    "        self.crop_1 = tf.keras.layers.Cropping2D(cropping=(1,1))\n",
    "\n",
    "        self.conv2d_1 = tf.keras.layers.Conv2D(num_classes, kernel_size=(1,1), activation='relu', padding='same')\n",
    "        self.add_1 = tf.keras.layers.Add()\n",
    "\n",
    "        self.x2_upsample_2 = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4,4), strides=(2,2), use_bias=False)\n",
    "        self.crop_2 = tf.keras.layers.Cropping2D(cropping=(1,1))\n",
    "\n",
    "        self.conv2d_2 = tf.keras.layers.Conv2D(num_classes, kernel_size=(1,1), activation='relu', padding='same')\n",
    "        self.add_2 = tf.keras.layers.Add()\n",
    "\n",
    "        self.x8_upsample = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(8,8), strides=(8,8), use_bias=False)\n",
    "        self.classifier = tf.keras.layers.Activation('softmax')\n",
    "        \n",
    "    def call(self, convs):\n",
    "        \n",
    "        # unpack the output of the encoder\n",
    "        f1, f2, f3, f4, f5 = convs\n",
    "\n",
    "        # upsample the output of the encoder then crop extra pixels that were introduced\n",
    "        o = self.x2_upsample_1(f5)\n",
    "        o = self.crop_1(o)\n",
    "\n",
    "        # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "        o2 = f4\n",
    "        o2 = self.conv2d_1(o2)\n",
    "\n",
    "        # add the results of the upsampling and pool 4 prediction\n",
    "        o = self.add_1([o, o2])\n",
    "\n",
    "        # upsample the resulting tensor of the operation you just did\n",
    "        o = self.x2_upsample_2(o)\n",
    "        o = self.crop_2(o)\n",
    "\n",
    "        # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "        o2 = f3\n",
    "        o2 = self.conv2d_2(o2)\n",
    "\n",
    "        # add the results of the upsampling and pool 3 prediction\n",
    "        o = self.add_2([o, o2])\n",
    "\n",
    "        # upsample up to the size of the original image\n",
    "        o = self.x8_upsample(o)\n",
    "        \n",
    "        # append a softmax to get the class probabilities\n",
    "        o = self.classifier(o)\n",
    "        return o "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d56863-e803-40ae-8aab-a9decad4a1ca",
   "metadata": {},
   "source": [
    "## Define Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc385da3-097f-4760-9170-1baaf22391db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_model():\n",
    "    '''\n",
    "    Defines the final segmentation model by chaining together the encoder and decoder.\n",
    "    \n",
    "    Returns:\n",
    "    keras Model that connects the encoder and decoder networks of the segmentation model\n",
    "    '''\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(224,224,3,))\n",
    "    vgg16_encoder = VGG_16()\n",
    "    \n",
    "    # load the pretrained weights you downloaded earlier\n",
    "    weights_path = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    vgg16_encoder.load_weights(weights_path, by_name=True)\n",
    "    (p1,p2,p3,p4,p5) = vgg16_encoder(inputs)\n",
    "    \n",
    "    # number of filters for the output convolutional layers\n",
    "    n = 4096\n",
    "    \n",
    "    # our input images are 224x224 pixels so they will be downsampled to 7x7 after the pooling layers above.\n",
    "    # we can extract more features by chaining two more convolution layers.\n",
    "    c6 = tf.keras.layers.Conv2D(n, kernel_size=(7,7), padding='same', activation='relu', name='conv6')(p5)\n",
    "    c7 = tf.keras.layers.Conv2D(n, kernel_size=(1,1), padding='same', activation='relu', name='conv7')(c6)\n",
    "    convs = (p1,p2,p3,p4,c7)\n",
    "    \n",
    "    fcn_decoder = FCN_8(12)\n",
    "    outputs = fcn_decoder(convs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954232f-04e2-4369-ad1d-5d864eee713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model and see how it looks\n",
    "model = segmentation_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a79b8-5c60-473a-8acb-a022a98abbdb",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ece97b-95c7-42e1-ae27-373a47d862f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=1E-2, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b552a-3afe-4ad9-83aa-2323ebcd898a",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "The model can now be trained after 170 epochs. This will take around 30 minutes to run and you will reach around 85% accuracy for both train and val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb94c5-75e1-4bb7-866b-92d3d21331d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training images\n",
    "train_count = 367\n",
    "\n",
    "# number of validation images\n",
    "validation_count = 101\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "steps_per_epoch = train_count//BATCH_SIZE\n",
    "validation_steps = validation_count//BATCH_SIZE\n",
    "\n",
    "history = model.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7adbb-c0dd-4e66-98c6-29f77c89e217",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b78064-e3cd-46de-8d5d-ceb62858b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_segments_test_arrays():\n",
    "  '''\n",
    "  Gets a subsample of the val set as your test set\n",
    "\n",
    "  Returns:\n",
    "    Test set containing ground truth images and label maps\n",
    "  '''\n",
    "  y_true_segments = []\n",
    "  y_true_images = []\n",
    "  test_count = 64\n",
    "\n",
    "  ds = validation_dataset.unbatch()\n",
    "  ds = ds.batch(101)\n",
    "\n",
    "  for image, annotation in ds.take(1):\n",
    "    y_true_images = image\n",
    "    y_true_segments = annotation\n",
    "\n",
    "\n",
    "  y_true_segments = y_true_segments[:test_count, : ,: , :]\n",
    "  y_true_segments = np.argmax(y_true_segments, axis=3)\n",
    "\n",
    "  return y_true_images, y_true_segments\n",
    "\n",
    "# load the ground truth images and segmentation masks\n",
    "y_true_images, y_true_segments = get_images_and_segments_test_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59c509-fb9d-4111-87f6-b72e8dd96291",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "You can get output segmentation masks by using the predict() method. As you may recall, the output of our segmentation model has the shape (height, width, 12) where 12 is the number of classes. Each pixel value in those 12 slices indicates the probability of that pixel belonging to that particular class. If you want to create the predicted label map, then you can get the argmax() of that axis. This is shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1aec7-c512-471e-bac9-604c0cde926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(validation_dataset, steps=validation_steps)\n",
    "results = np.argmax(results, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e118987-5c7d-4612-ae0d-5c0dc665caa8",
   "metadata": {},
   "source": [
    "### Compute Metrics\n",
    "\n",
    "The function below generates the IOU and dice score of the prediction and ground truth masks. From the lectures, it is given that:\n",
    "\n",
    "$$IOU = \\frac{area\\_of\\_overlap}{area\\_of\\_union}$$\n",
    "<br>\n",
    "$$Dice Score = 2 * \\frac{area\\_of\\_overlap}{combined\\_area}$$\n",
    "\n",
    "The code below does that for you. A small smoothening factor is introduced in the denominators to prevent possible division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3160a5-d583-4e02-9858-915b0ccfe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    class_wise_iou = []\n",
    "    class_wise_dice_score= []\n",
    "    smoothening_factor = 0.00001\n",
    "    for i in range(12):\n",
    "        intersection = np.sum((y_pred==i) * (y_true==i))\n",
    "        y_true_area = np.sum(y_true==i)\n",
    "        y_pred_area = np.sum(y_pred==i)\n",
    "        combined_area = y_true_area + y_pred_area\n",
    "        iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
    "        class_wise_iou.append(iou)\n",
    "        dice_score = (2 * intersection + smoothening_factor) / (combined_area + smoothening_factor)\n",
    "        class_wise_dice_score.append(dice_score)\n",
    "\n",
    "    return class_wise_iou, class_wise_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbd838-2557-4b46-b9c8-6aeb199b824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input a number from 0 to 63 to pick an image from the test set\n",
    "integer_slider = 0\n",
    "\n",
    "# compute metrics\n",
    "iou, dice_score = compute_metrics(y_true_segments[integer_slider], results[integer_slider])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda0935-a92d-45b4-99e6-ec2e45cb33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(y_true_images[integer_slider], [results[integer_slider], y_true_segments[integer_slider]], [\"Image\", \"Predicted Mask\", \"True Mask\"], iou, dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcad9dd-f32d-431b-a51a-e0cdb752ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class-wise metrics\n",
    "cls_wise_iou, cls_wise_dice_score = compute_metrics(y_true_segments, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991f041-8627-4cb1-9d90-6e59f026bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print IOU for each class\n",
    "for idx, iou in enumerate(cls_wise_iou):\n",
    "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
    "  print(\"{}{}{} \".format(class_names[idx], spaces, iou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425de83b-a29c-4beb-ac26-21ea69aafbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dice score for each class\n",
    "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
    "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
    "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9525a-3c58-42f4-bc27-6071f5a53a5a",
   "metadata": {},
   "source": [
    "### Thats All for it Today, I have Trained the following model for more then 170 epochs on collab, and the results were pretty satisfactory "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
